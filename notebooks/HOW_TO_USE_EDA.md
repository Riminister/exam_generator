# How to Use Exploratory Data Analysis - Step-by-Step Guide

## ğŸ¯ Purpose of Exploratory Data Analysis (EDA)

**Before building ML models**, you need to:
1. **Understand your data** - What do you have? Is it clean?
2. **Identify patterns** - Are there relationships between features?
3. **Find issues** - Missing data, outliers, class imbalance?
4. **Make informed decisions** - What models make sense? What features to use?

This notebook helps you do all of that!

---

## ğŸ“‹ Step-by-Step Process

### **Step 1: Run the Notebook**

#### Option A: Using Jupyter Notebook
```bash
# In your terminal (from project root)
jupyter notebook notebooks/exploratory_analysis.ipynb
```

#### Option B: Using VS Code or PyCharm
- Just open `notebooks/exploratory_analysis.ipynb` in your IDE
- Click "Run All" or run cells one by one (Shift+Enter)

#### Option C: Using JupyterLab
```bash
jupyter lab notebooks/exploratory_analysis.ipynb
```

---

### **Step 2: Understand Each Section**

#### **Section 1: Setup and Imports** âœ…
- **What it does**: Loads libraries (pandas, matplotlib, etc.)
- **What to check**: Make sure all imports succeed (no errors)
- **Action**: If errors occur, install missing packages: `pip install pandas matplotlib seaborn numpy`

#### **Section 2: Load Data** ğŸ“Š
- **What it does**: Loads your `data/exam_analysis.json` file
- **What to look for**: 
  - âœ… Should show number of exams loaded
  - âœ… Should show total questions
- **If it fails**: Check that `data/exam_analysis.json` exists

#### **Section 3: Convert to DataFrame** ğŸ”„
- **What it does**: Converts JSON to a pandas DataFrame (table)
- **What to look for**: 
  - DataFrame shape (rows Ã— columns)
  - List of columns available
  - Preview of first few rows
- **Action**: Review the columns - these are your **features** for ML models!

#### **Section 4: Basic Statistics** ğŸ“ˆ
- **What it shows**:
  - Total questions, exams, courses
  - Question type distribution
  - Course distribution
- **What to look for**:
  - âœ… Are there enough questions? (You have 201 - good for starting!)
  - âœ… Are question types balanced? (If one type has 90%, that's imbalanced)
  - âœ… How many courses? (13 courses - good diversity)
- **Action**: Note any class imbalance - you'll need to handle this in models

**Example Interpretation**:
```
Total Questions: 201
Question Types:
  essay: 51 (25%)     â† Good balance
  other: 83 (41%)     â† Largest category
  short_answer: 38 (19%)
```

#### **Section 5: Question Type Visualization** ğŸ“Š
- **What it shows**: Bar charts and pie charts of question types
- **What to look for**:
  - **Class imbalance**: If one type dominates (>60%), use class weights in models
  - **Missing types**: Are expected types missing?
- **Action**: 
  - If imbalanced: Plan to use `class_weight='balanced'` in sklearn models
  - Note which types you want to predict

#### **Section 6: Question Length Analysis** ğŸ“
- **What it shows**: 
  - Distribution of question lengths
  - Length by question type
  - Violin plots showing spread
- **What to look for**:
  - **Outliers**: Very long questions (>5000 chars) might be errors
  - **Patterns**: Do essay questions tend to be longer? (Probably yes!)
  - **Normal distribution?**: If skewed, use log transform
- **Action**:
  - Very short questions (<20 chars): Review for data quality issues
  - Very long questions: Might need special handling
  - **Feature idea**: Question length is a good feature for classification!

#### **Section 7: Difficulty Score Analysis** ğŸ¯
- **What it shows**:
  - Distribution of difficulty scores
  - Difficulty by question type
  - Relationship between length and difficulty
- **What to look for**:
  - **Coverage**: How many questions have difficulty > 0? 
    - If <50%, difficulty prediction might be hard
  - **Correlation**: Does longer = more difficult?
    - If correlation > 0.3, length is a useful feature
  - **Patterns**: Do certain question types have higher difficulty?
- **Action**:
  - If many zeros: Difficulty prediction might need more data or better features
  - If good coverage: You can build a difficulty prediction model
  - Use difficulty as a feature for other models

#### **Section 8: Data Quality Checks & Summary** ğŸ”
- **What it shows**:
  - Missing values
  - Short questions (potential errors)
  - Zero difficulty scores
  - Topic coverage
  - Summary recommendations
- **What to look for**:
  - âŒ **Missing values**: Need to handle (drop or impute)
  - âš ï¸ **Short questions**: Might be noise or incomplete
  - âš ï¸ **Zero difficulty**: Need assessment or remove from difficulty model
  - âš ï¸ **Low topic coverage**: Consider running topic extraction
- **Action**: Based on findings, decide:
  - Run data cleaning? (YES - recommended!)
  - Build models now? (YES if data looks good)
  - Need more data? (Depends on quality)

---

## ğŸ¯ What to Do Based on Findings

### **Scenario 1: Data Looks Good** âœ…
If you see:
- âœ… No missing values
- âœ… Balanced question types
- âœ… Good difficulty coverage (>50%)
- âœ… Reasonable length distribution

**Next Steps**:
1. âœ… Run data cleaning: `python exam_analysis/run_cleaning.py`
2. âœ… Build first model: `python models/build_first_model.py`
3. âœ… Evaluate results
4. âœ… Iterate and improve

### **Scenario 2: Class Imbalance** âš ï¸
If one question type dominates (>60%):

**Actions**:
- Use `class_weight='balanced'` in sklearn models
- Use stratified train/test split
- Consider SMOTE for oversampling (if needed)
- Focus on most common types first

### **Scenario 3: Poor Data Quality** âŒ
If you find:
- Many missing values
- Many very short/long questions
- Low difficulty coverage

**Actions**:
1. **Run data cleaning FIRST**: `python exam_analysis/run_cleaning.py`
2. Review cleaned data
3. Decide if you need more data
4. Then build models

### **Scenario 4: Small Dataset** ğŸ“‰
If you have <100 questions:

**Actions**:
- Start with simple models (logistic regression)
- Use cross-validation (not just train/test split)
- Consider transfer learning (pre-trained models)
- Plan to download more exams later

---

## ğŸ“Š Key Metrics to Watch

### **For Classification Models**:
1. **Class balance**: Is one type >60%? â†’ Use class weights
2. **Feature availability**: Do you have text + metadata? â†’ Good for models
3. **Sample size**: <50 per class? â†’ Use simple models first

### **For Regression Models** (Difficulty):
1. **Coverage**: >50% have difficulty scores? â†’ Can build model
2. **Distribution**: Normal or skewed? â†’ Use appropriate metrics
3. **Correlations**: Length correlates with difficulty? â†’ Use as feature

---

## ğŸ”„ Iterative Process

EDA is not one-time! Do it:
1. **Before cleaning** - Understand raw data
2. **After cleaning** - Verify improvements
3. **After model training** - Understand errors
4. **When adding new data** - Check changes

---

## ğŸ’¡ Pro Tips

1. **Save outputs**: The notebook can save summary CSVs - use them!
2. **Modify parameters**: Change thresholds, colors, bins to explore
3. **Ask questions**: Why is X correlated with Y? What does this mean?
4. **Document findings**: Take notes on what you discover
5. **Compare before/after**: Run EDA before and after cleaning

---

## ğŸ¯ Quick Decision Tree

```
Run EDA Notebook
â”‚
â”œâ”€ Data looks clean? 
â”‚  â”œâ”€ YES â†’ Run cleaning â†’ Build models
â”‚  â””â”€ NO â†’ Run cleaning â†’ Re-run EDA â†’ Build models
â”‚
â”œâ”€ Enough data?
â”‚  â”œâ”€ YES (>100 questions) â†’ Build models
â”‚  â””â”€ NO (<100 questions) â†’ Use simple models or get more data
â”‚
â”œâ”€ Class imbalance?
â”‚  â”œâ”€ YES â†’ Use class weights, stratified splits
â”‚  â””â”€ NO â†’ Standard train/test split is fine
â”‚
â””â”€ Difficulty coverage good?
   â”œâ”€ YES (>50%) â†’ Build difficulty model
   â””â”€ NO (<50%) â†’ Skip difficulty model, focus on classification
```

---

## ğŸ“ Checklist After Running EDA

- [ ] Reviewed all visualizations
- [ ] Noted any data quality issues
- [ ] Identified class imbalances
- [ ] Checked feature distributions
- [ ] Reviewed summary recommendations
- [ ] Decided on next steps (clean? model? get more data?)
- [ ] Documented key findings

---

## ğŸš€ Next Steps After EDA

1. **Clean the data**: `python exam_analysis/run_cleaning.py`
2. **Review cleaned data**: Run EDA again on cleaned data
3. **Build first model**: `python models/build_first_model.py`
4. **Evaluate results**: Did model performance match EDA insights?
5. **Iterate**: Improve models based on findings

---

## â“ Common Questions

**Q: Do I need to understand every chart?**
A: No! Focus on the summary and recommendations. Charts are visual aids.

**Q: What if I find errors?**
A: That's great! EDA found issues before they hurt your models. Fix them first.

**Q: How long should EDA take?**
A: First time: 30-60 min. Subsequent runs: 10-15 min.

**Q: Should I run EDA after every change?**
A: Yes, especially after cleaning or adding new data.

---

## ğŸ“ Learning Resources

- **Pandas**: https://pandas.pydata.org/docs/
- **Matplotlib**: https://matplotlib.org/stable/tutorials/
- **Seaborn**: https://seaborn.pydata.org/tutorial.html

---

**Remember**: EDA is exploration - there are no wrong answers, only discoveries! ğŸ”âœ¨


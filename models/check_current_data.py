#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""Quick script to analyze current dataset before building models"""

import json
import sys
import os
from pathlib import Path
from collections import Counter

# Fix encoding for Windows
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

def analyze_dataset(json_file="data/exam_analysis.json"):
    """Analyze the current dataset and provide recommendations"""
    
    if not Path(json_file).exists():
        print(f"‚ùå {json_file} not found!")
        return
    
    with open(json_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    exams = data.get('exams', [])
    
    if not exams:
        print("‚ùå No exams found in dataset!")
        return
    
    # Gather statistics
    total_questions = []
    question_types = []
    courses = []
    topics = []
    
    for exam in exams:
        questions = exam.get('questions', [])
        total_questions.extend(questions)
        courses.append(exam.get('course_code', 'unknown'))
        for q in questions:
            question_types.append(q.get('question_type', 'unknown'))
            topics.extend(q.get('topics', []))
    
    # Print analysis
    print("=" * 70)
    print("üìä CURRENT DATASET ANALYSIS")
    print("=" * 70)
    print(f"\n‚úÖ Total Exams: {len(exams)}")
    print(f"‚úÖ Total Questions: {len(total_questions)}")
    print(f"‚úÖ Unique Courses: {len(set(courses))}")
    print(f"   Courses: {', '.join(sorted(set(courses)))}")
    
    print(f"\nüìù Question Types:")
    type_counts = Counter(question_types)
    for qtype, count in type_counts.most_common():
        print(f"   - {qtype}: {count} ({count/len(question_types)*100:.1f}%)")
    
    print(f"\nüè∑Ô∏è  Topics:")
    topic_counts = Counter(topics)
    if topic_counts:
        for topic, count in topic_counts.most_common(10):
            print(f"   - {topic}: {count}")
    else:
        print("   (No topics extracted yet)")
    
    # Calculate average questions per exam
    avg_q_per_exam = len(total_questions) / len(exams) if exams else 0
    print(f"\nüìà Average Questions per Exam: {avg_q_per_exam:.1f}")
    
    # Recommendations
    print("\n" + "=" * 70)
    print("üí° RECOMMENDATIONS FOR MODEL BUILDING")
    print("=" * 70)
    
    if len(total_questions) < 100:
        print("\n‚ö†Ô∏è  Dataset is SMALL (<100 questions)")
        print("   ‚Üí Start with simple models first:")
        print("      ‚Ä¢ Basic classification (question type)")
        print("      ‚Ä¢ Simple rule-based difficulty assessment")
        print("      ‚Ä¢ Test data cleaning pipeline")
        print("\n   ‚Üí Download more exams AFTER validating approach")
    elif len(total_questions) < 500:
        print("\n‚úÖ Dataset is MEDIUM (100-500 questions)")
        print("   ‚Üí Good for initial model testing:")
        print("      ‚Ä¢ Question classification models")
        print("      ‚Ä¢ Topic extraction/clustering")
        print("      ‚Ä¢ Basic difficulty prediction")
        print("\n   ‚Üí Can start model development, but consider:")
        print("      ‚Ä¢ Downloading more data for better generalization")
        print("      ‚Ä¢ Focus on one course first to understand patterns")
    else:
        print("\n‚úÖ Dataset is LARGE (500+ questions)")
        print("   ‚Üí Ready for serious model development:")
        print("      ‚Ä¢ Train classification models")
        print("      ‚Ä¢ Build topic models (LDA, etc.)")
        print("      ‚Ä¢ Fine-tune language models")
        print("\n   ‚Üí Consider downloading more for:")
        print("      ‚Ä¢ Better generalization across courses")
        print("      ‚Ä¢ More diverse question types")
    
    print("\nüéØ RECOMMENDED NEXT STEPS:")
    print("   1. Run data cleaning: python exam_analysis/data_cleaner.py")
    print("   2. Build exploratory analysis notebook")
    print("   3. Start with simple ML models (classification, clustering)")
    print("   4. Test approach on this dataset")
    print("   5. If results look good ‚Üí download more exams")
    print("   6. If issues found ‚Üí fix pipeline first, then scale up")
    
    print("\n" + "=" * 70)

if __name__ == "__main__":
    analyze_dataset()

